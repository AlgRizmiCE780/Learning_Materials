# Deep Learning ‚Üí LLM ‚Üí GenAI Prerequisites Roadmap

## üìä How to Use This Guide

**Depth Indicators:**
- ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê **Critical** - Master deeply, cannot skip
- ‚≠ê‚≠ê‚≠ê‚≠ê **Very Important** - Strong understanding needed
- ‚≠ê‚≠ê‚≠ê **Important** - Conceptual understanding required
- ‚≠ê‚≠ê **Useful** - Good to know, learn as needed
- ‚≠ê **Awareness** - Surface-level knowledge sufficient

**Time Estimates:** Approximate hours for someone with math background
- Assumes focused study time
- Adjust based on your prior knowledge

---

## Table of Contents
1. [Mathematical Foundations](#1-mathematical-foundations)
2. [Programming & Data Fundamentals](#2-programming--data-fundamentals)
3. [Classical Machine Learning Essentials](#3-classical-machine-learning-essentials)
4. [Deep Learning Core Concepts](#4-deep-learning-core-concepts)
5. [Architecture-Specific Prerequisites](#5-architecture-specific-prerequisites)
6. [Hardware & Systems Awareness](#6-hardware--systems-awareness)

---

## 1. Mathematical Foundations
**Total Time Estimate: 15-25 hours** (refresher) or **40-60 hours** (from scratch)

### 1.1 Linear Algebra ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Cannot Skip)
**Depth:** Master deeply | **Time:** 8-12 hours (refresher) or 20-30 hours (from scratch)  
**Why:** Neural networks are matrix operations at their core. Every layer is a matrix multiplication.

**Core Subtopics:**

1. **Vector & Matrix Operations** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)
   - Vector addition, scalar multiplication
   - Dot product (geometric intuition + formula)
   - Matrix multiplication (practice 3x3 by hand)
   - Transpose, inverse (when it exists)
   - **Practice:** Multiply 2 matrices in NumPy, understand shapes

2. **Matrix Decomposition** (‚≠ê‚≠ê‚≠ê‚≠ê)
   - **SVD** (Singular Value Decomposition): $A = U\Sigma V^T$
     - Used in: PCA, low-rank approximations, recommender systems
   - **Eigenvalues & Eigenvectors**: $Av = \lambda v$
     - Used in: PCA, understanding attention mechanisms
   - **Don't memorize proofs** - understand when/why to use

3. **Norms & Distances** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)
   - **L1 norm** (Manhattan): $\|x\|_1 = \sum |x_i|$ ‚Üí Sparse solutions
   - **L2 norm** (Euclidean): $\|x\|_2 = \sqrt{\sum x_i^2}$ ‚Üí Ridge regression
   - **Dot product & Cosine similarity**: $\cos(\theta) = \frac{x \cdot y}{\|x\|\|y\|}$
     - Critical for: Word embeddings, attention scores

4. **Tensor Operations** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)
   - **Broadcasting**: Adding (128, 768) + (768,) ‚Üí (128, 768)
   - **Reshaping**: (batch, seq, features) transformations
   - **Einstein summation** (einsum): Compact tensor operations
   - **Practice:** Implement matrix multiplication using einsum

**Learning Path:**
- Week 1: 3Blue1Brown "Essence of Linear Algebra" (visual intuition)
- Week 2: Gilbert Strang MIT lectures (Lectures 1-10)
- Week 2-3: NumPy practice - implement matrix ops without using `@`

**Skip for Now:**
- Jordan canonical form
- Abstract vector space proofs
- Detailed decomposition algorithm proofs

---

### 1.2 Calculus ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Cannot Skip)
**Depth:** Master deeply | **Time:** 6-10 hours (refresher) or 15-25 hours (from scratch)  
**Why:** Backpropagation = repeated chain rule application. This is THE core of neural network training.

**Core Subtopics:**

1. **Derivatives & Partial Derivatives** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)
   - Single-variable derivatives: $\frac{d}{dx}(x^2) = 2x$
   - **Partial derivatives**: $\frac{\partial}{\partial x}f(x,y)$ - holding other variables constant
   - **Geometric intuition**: Slope, rate of change, tangent line
   - **Practice:** Compute $\frac{\partial}{\partial w}(wx + b)^2$

2. **Chain Rule** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)
   - Single-variable: $\frac{d}{dx}f(g(x)) = f'(g(x)) \cdot g'(x)$
   - **Multi-variable chain rule**: Critical for backprop
   - **Computational graphs**: Visualizing nested functions
   - **Example:** $L = (Wx + b - y)^2$ ‚Üí find $\frac{\partial L}{\partial W}$

3. **Gradients** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)
   - **Gradient vector**: $\nabla f = [\frac{\partial f}{\partial x_1}, ..., \frac{\partial f}{\partial x_n}]$
   - **Direction of steepest ascent**
   - Negative gradient = steepest descent (optimization)
   - **Practice:** Compute gradient of $f(x, y) = x^2 + xy + y^2$

4. **Jacobian & Hessian Matrices** (‚≠ê‚≠ê‚≠ê)
   - **Jacobian**: Matrix of all first-order partial derivatives
     - Used in: Backprop through vector-valued functions
   - **Hessian**: Matrix of second-order derivatives
     - Used in: Understanding curvature, advanced optimizers (Newton's method)
   - **Don't deep-dive** - understand conceptually

5. **Matrix Calculus** (‚≠ê‚≠ê‚≠ê‚≠ê)
   - $\frac{\partial \mathbf{y}}{\partial \mathbf{x}}$ where both are vectors
   - **Key formulas to memorize:**
     - $\frac{\partial}{\partial x}(Wx) = W^T$
     - $\frac{\partial}{\partial W}(Wx) = x^T$
   - **Practice:** Derive gradient of linear regression loss

**Learning Path:**
- 3Blue1Brown "Essence of Calculus" (3-4 hours)
- Khan Academy multivariable calculus (chain rule section)
- **Hands-on:** Implement backprop for 2-layer network by hand

**Skip for Now:**
- Integration (rarely needed in DL)
- Differential equations (unless research)
- Taylor series (know it exists, don't memorize)

---

### 1.3 Probability & Statistics ‚≠ê‚≠ê‚≠ê‚≠ê
**Depth:** Strong conceptual understanding | **Time:** 6-10 hours  
**Why:** Models output probabilities. Loss functions measure distributional differences.

**Core Subtopics:**

1. **Probability Fundamentals** (‚≠ê‚≠ê‚≠ê‚≠ê)
   - Sample space, events, probability axioms
   - **Conditional probability**: $P(A|B) = \frac{P(A \cap B)}{P(B)}$
   - **Bayes' Theorem**: $P(A|B) = \frac{P(B|A)P(A)}{P(B)}$
     - Foundation of probabilistic reasoning

2. **Probability Distributions** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)
   - **Gaussian/Normal**: $\mathcal{N}(\mu, \sigma^2)$
     - Most common in DL (weight initialization, noise)
   - **Bernoulli**: Binary outcomes (single coin flip)
   - **Categorical**: Multi-class (dice roll, image classification)
   - **Understanding**: Mean, variance, standard deviation, PDF vs PMF

3. **Maximum Likelihood Estimation (MLE)** (‚≠ê‚≠ê‚≠ê‚≠ê)
   - **Core idea**: Find parameters that maximize $P(\text{data}|\theta)$
   - **Connection to DL**: Training = MLE (minimizing negative log-likelihood)
   - **Example:** MLE for Gaussian ‚Üí mean and variance formulas

4. **Information Theory** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)
   - **Entropy**: $H(X) = -\sum p(x) \log p(x)$
     - Measures uncertainty/surprise
   - **Cross-Entropy**: $H(p, q) = -\sum p(x) \log q(x)$
     - **THE classification loss function**
   - **KL Divergence**: $D_{KL}(p||q) = \sum p(x) \log \frac{p(x)}{q(x)}$
     - Measures distributional difference
     - Used in: VAEs, RLHF reward modeling

5. **Statistical Concepts** (‚≠ê‚≠ê‚≠ê‚≠ê)
   - **Expectation**: $E[X] = \sum x \cdot p(x)$
   - **Variance**: $Var(X) = E[(X - E[X])^2]$
   - **Bias-Variance Tradeoff**: Underfitting vs overfitting
     - High bias = too simple (underfits)
     - High variance = too complex (overfits)

**Learning Path:**
- Harvard Stat 110 lectures 1-5 (Joe Blitzstein)
- Khan Academy probability course
- **Derive cross-entropy loss** from MLE yourself

**Skip for Now:**
- Hypothesis testing (t-tests, p-values)
- Confidence intervals
- Advanced statistical inference

---

### 1.4 Optimization Theory ‚≠ê‚≠ê‚≠ê‚≠ê
**Depth:** Strong understanding | **Time:** 4-6 hours  
**Why:** Training = solving a non-convex optimization problem with ~100M+ variables.

**Core Subtopics:**

1. **Optimization Landscape** (‚≠ê‚≠ê‚≠ê‚≠ê)
   - **Convex functions**: Single global minimum (easy)
   - **Non-convex functions**: Multiple local minima, saddle points (neural networks)
   - **Loss landscape visualization**: Hills, valleys, flat regions

2. **Gradient Descent Variants** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)
   - **Batch GD**: Use entire dataset ‚Üí slow, stable
   - **Stochastic GD (SGD)**: Use 1 sample ‚Üí fast, noisy
   - **Mini-batch GD**: Use batch (32-256) ‚Üí best of both
   - **Why randomness helps**: Escape local minima, approximate gradients

3. **Critical Concepts** (‚≠ê‚≠ê‚≠ê‚≠ê)
   - **Local minima vs saddle points**: Saddle points are the real problem in high dimensions
   - **Flat vs sharp minima**: Flat minima generalize better
   - **Learning rate**: Too high = diverge, too low = slow convergence
   - **Convergence**: When to stop training

**Learning Path:**
- Watch 1-2 videos on gradient descent visualization
- **Implement** gradient descent on simple 2D function
- Understand intuitively, not mathematically (save for DL phase)

**Add Later (During DL Phase):**
- Momentum, RMSProp, Adam mathematics
- Second-order methods (Newton's method, L-BFGS)
- Learning rate schedules

---

## 2. Programming & Data Fundamentals
**Total Time Estimate: 10-20 hours** (if you know Python) or **40-60 hours** (Python beginner)

### 2.1 Python Programming ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Critical)
**Depth:** Strong working knowledge | **Time:** 8-15 hours (refresher) or 30-50 hours (beginner)  
**Why:** 99% of DL/ML uses Python. PyTorch and TensorFlow are Python libraries.

**Core Subtopics:**

1. **Python Fundamentals** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)
   - **Data structures**: Lists, tuples, dictionaries, sets
   - **Control flow**: if/else, for loops, while loops
   - **Functions**: Define, call, return values, *args, **kwargs
   - **List comprehensions**: `[x**2 for x in range(10)]`
   - **Classes & OOP basics**: __init__, methods, inheritance (light touch)

2. **NumPy Mastery** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)
   - **Array creation**: `np.array()`, `np.zeros()`, `np.random.randn()`
   - **Indexing & slicing**: `arr[1:5]`, `arr[:, 0]`, boolean indexing
   - **Broadcasting**: Automatic shape matching
   - **Reshaping**: `.reshape()`, `.transpose()`, `.flatten()`
   - **Operations**: Element-wise, matrix multiplication `@`
   - **Practice:**
     - Implement matrix multiplication without `@`
     - Normalize a matrix to mean=0, std=1
     - Compute softmax from scratch

3. **Pandas Basics** (‚≠ê‚≠ê‚≠ê)
   - **DataFrames**: Loading CSV, basic operations
   - **Data cleaning**: Handling missing values, filtering
   - **Column operations**: Select, add, transform
   - **Purpose:** Prepare data before feeding to models

4. **Matplotlib/Seaborn** (‚≠ê‚≠ê‚≠ê)
   - **Line plots**: For loss curves
   - **Scatter plots**: For visualizing embeddings
   - **Histograms**: Distribution of activations
   - **Heatmaps**: Confusion matrices, attention weights

**Learning Path:**
- Python basics: Kaggle Python course (if needed)
- NumPy: "NumPy Tutorial" by Nicolas Rougier
- **Daily practice:** 30min coding challenges (LeetCode Easy problems)

**Good to Have (‚≠ê‚≠ê):**
- Decorators (`@property`, `@staticmethod`)
- Generators (memory efficiency)
- Type hints (`def func(x: int) -> float`)

---

### 2.2 Software Engineering Basics ‚≠ê‚≠ê‚≠ê
**Depth:** Practical working knowledge | **Time:** 4-8 hours  
**Why:** ML projects become unwieldy without proper code organization.

**Core Subtopics:**

1. **Git Version Control** (‚≠ê‚≠ê‚≠ê‚≠ê)
   - **Basic commands**:
     - `git init`, `git add`, `git commit -m "message"`
     - `git push`, `git pull`, `git clone`
   - **Branching**: `git branch`, `git checkout`, `git merge`
   - **Purpose:** Track experiments, collaborate, undo mistakes
   - **Practice:** Create a repo, make 5 commits, create a branch

2. **Virtual Environments** (‚≠ê‚≠ê‚≠ê‚≠ê)
   - **Conda**: `conda create -n myenv python=3.10`
   - **venv**: `python -m venv myenv`
   - **Why:** Isolate dependencies (PyTorch 2.0 vs 1.13)
   - **Practice:** Create env, install packages, export to requirements.txt

3. **Code Organization** (‚≠ê‚≠ê‚≠ê)
   - **Functions**: Reusable logic blocks
   - **Modules**: Separate files (e.g., `model.py`, `train.py`, `utils.py`)
   - **Classes**: For models, datasets
   - **Purpose:** Readable, maintainable code

4. **Debugging** (‚≠ê‚≠ê‚≠ê)
   - **Print debugging**: Strategic `print()` statements
   - **Python debugger (pdb)**: Breakpoints, step through code
   - **Understanding stack traces**: Reading error messages
   - **Practice:** Debug a broken neural network script

**Learning Path:**
- Git: "Git and GitHub for Beginners" (freeCodeCamp)
- Practice: Set up a project structure for your first DL experiment

**Skip for Now:**
- Advanced design patterns (Factory, Singleton)
- CI/CD pipelines
- Docker, Kubernetes (MLOps - learn later)

---

## 3. Classical Machine Learning Essentials
**Total Time Estimate: 8-12 hours**

### 3.1 Core ML Concepts ‚≠ê‚≠ê‚≠ê‚≠ê (Don't Skip)
**Depth:** Strong conceptual understanding | **Time:** 6-10 hours  
**Why:** DL is an extension of ML. Understanding fundamentals prevents cargo-culting and helps debug.

**Core Subtopics:**

1. **Learning Paradigms** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)
   
   **A. Supervised Learning** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)  
   **Depth:** Deep understanding | **Time:** 3-4 hours  
   **Why:** 90% of DL/LLM work is supervised learning
   
   - **Definition**: Learning from labeled data (X, Y) pairs
   - **Types:**
     - **Classification**: Input ‚Üí Discrete label (image ‚Üí cat/dog)
     - **Regression**: Input ‚Üí Continuous value (house features ‚Üí price)
   - **How it works**: Minimize difference between prediction and label
   - **In DL/LLMs:**
     - Image classification (ResNet on ImageNet)
     - **Next token prediction** (LLMs are supervised!)
     - Named entity recognition
   
   **Subtopics to Master:**
   - Loss functions (MSE for regression, cross-entropy for classification)
   - How labels guide learning
   - Difference from unsupervised (you have "answers")
   
   ---
   
   **B. Unsupervised Learning** (‚≠ê‚≠ê‚≠ê)  
   **Depth:** Conceptual understanding | **Time:** 2-3 hours  
   **Why:** Pre-training, embeddings, dimensionality reduction
   
   - **Definition**: Learning from unlabeled data (only X, no Y)
   - **Goal**: Find patterns, structure, representations
   - **Classical methods** (survey only):
     - **Clustering**: k-means, hierarchical (grouping similar items)
     - **Dimensionality reduction**: PCA, t-SNE (visualizing high-D data)
   - **Modern connection:**
     - **Self-supervised learning** (the big one!):
       - Model creates its own labels from data structure
       - **Example**: Predict next word (LLM pre-training)
       - **Example**: Predict masked word (BERT)
       - Technically unsupervised, but uses clever "pseudo-labels"
   
   **Subtopics to Know:**
   - No explicit labels provided
   - Learn useful representations
   - **Self-supervised ‚â† unsupervised** (modern term, very important)
   - Connection to LLM pre-training
   
   **Don't implement:** k-means, PCA from scratch (use sklearn to see them once)
   
   ---
   
   **C. Reinforcement Learning** (‚≠ê)  
   **Depth:** Awareness only | **Time:** 1 hour  
   **Why:** RLHF (RL from Human Feedback) used in ChatGPT, but you won't implement RL
   
   - **Definition**: Agent learns by trial-and-error in an environment
   - **Components:**
     - **Agent**: The learner (e.g., LLM)
     - **Environment**: The world
     - **State**: Current situation
     - **Action**: What agent can do
     - **Reward**: Feedback signal (+1 for good, -1 for bad)
   - **Goal**: Maximize cumulative reward over time
   
   **Connection to LLMs:**
   - **RLHF** (RL from Human Feedback):
     - Humans rank model outputs (good/bad)
     - Train reward model from rankings
     - Use PPO (policy optimization) to improve LLM
     - Used to make ChatGPT helpful/harmless
   
   **What you need:**
   - Understand the concept (not the math)
   - Know RL is used in RLHF phase
   - Don't implement: Q-learning, policy gradients, TD-learning
   
   **When to learn more:**
   - If specializing in RLHF research
   - After mastering supervised DL/LLMs
   
   **Resources:**
   - Watch: "Reinforcement Learning in 10 minutes" (YouTube)
   - Read: 1 blog post on RLHF (Hugging Face blog has good one)

2. **Data Splitting** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)
   - **Train set (70-80%)**: Learn patterns
   - **Validation set (10-15%)**: Tune hyperparameters, early stopping
   - **Test set (10-15%)**: Final evaluation (touch ONLY once!)
   - **Why crucial**: Prevents overfitting, measures generalization
   - **Cross-validation**: k-fold (mostly for small datasets)

3. **Overfitting & Underfitting** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)
   - **Underfitting**: Model too simple, high training error (high bias)
   - **Overfitting**: Model too complex, memorizes training data (high variance)
   - **Goldilocks zone**: Generalizes to new data
   - **How to detect**: Training loss ‚Üì but validation loss ‚Üë
   - **Solutions**: Regularization, more data, simpler model, early stopping

4. **Regularization** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)
   - **L1 (Lasso)**: $\lambda \sum |w_i|$ ‚Üí Sparse weights (feature selection)
   - **L2 (Ridge)**: $\lambda \sum w_i^2$ ‚Üí Small weights (most common)
   - **Purpose**: Penalize complexity, prevent overfitting
   - **In DL**: Weight decay, dropout, batch norm (all forms of regularization)

5. **Feature Engineering Intuition** (‚≠ê‚≠ê‚≠ê)
   - **Classical ML**: Manually create features (e.g., "age √ó income")
   - **Deep Learning**: Model learns features automatically
   - **Why know this**: Understand what DL automates
   - **Example**: CNNs learn edge detectors (no manual feature extraction)

6. **Evaluation Metrics** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)
   
   **Classification:**
   - **Accuracy**: Correct predictions / Total
   - **Precision**: True Positives / (True Positives + False Positives)
   - **Recall**: True Positives / (True Positives + False Negatives)
   - **F1 Score**: Harmonic mean of Precision & Recall
   - **AUC-ROC**: Area under ROC curve (good for imbalanced data)
   - **When to use what**: Imbalanced data ‚Üí use F1/AUC, not accuracy
   
   **Regression:**
   - **MSE**: Mean Squared Error (penalizes large errors heavily)
   - **RMSE**: Root MSE (same units as target)
   - **MAE**: Mean Absolute Error (robust to outliers)
   - **R¬≤**: Proportion of variance explained

**Learning Path:**
- Andrew Ng's ML course (Coursera) - Weeks 1-3
- Fast.ai Practical ML for Coders (Lessons 1-2)
- **Hands-on**: Train logistic regression on Titanic dataset (Kaggle)

---

### 3.2 Classical Algorithms ‚≠ê‚≠ê (Quick Survey Only)
**Depth:** Surface understanding | **Time:** 2-3 hours  
**Why:** Understand the progression from simple to complex models, appreciate what DL solves.

**Survey Topics (Don't Deep-Dive):**

1. **Linear/Logistic Regression** (‚≠ê‚≠ê‚≠ê)
   - **Linear**: Predicts continuous value, $y = wx + b$
   - **Logistic**: Predicts probability, uses sigmoid
   - **Why important**: Understand loss function (MSE, cross-entropy) and gradient descent
   - **Limitation**: Cannot learn non-linear patterns

2. **Decision Trees** (‚≠ê‚≠ê)
   - **How**: Recursive splitting based on features
   - **Feature importance**: Which features matter most
   - **Limitation**: Overfits easily, not great for images/text

3. **k-Nearest Neighbors (k-NN)** (‚≠ê‚≠ê)
   - **How**: Classify based on k closest training examples
   - **Distance metrics**: Euclidean, cosine
   - **Curse of dimensionality**: Fails in high dimensions (images have 50k+ dimensions)

4. **PCA (Dimensionality Reduction)** (‚≠ê‚≠ê)
   - **Purpose**: Reduce dimensions while preserving variance
   - **How**: Find principal components (directions of max variance)
   - **Uses**: Data visualization, preprocessing

**What You Actually Need:**
- Understand **why** these methods fail on complex data:
  - Can't handle raw images (need manual feature extraction)
  - Can't learn hierarchical features
  - Struggle with high-dimensional data
- Appreciate **what** deep learning solves:
  - Automatic feature learning
  - Hierarchical representations
  - Scalability to big data

**Time Allocation:**
- Spend 30 minutes on each algorithm (just understand concept)
- Use sklearn to run each once
- **Don't**: Implement from scratch, tune hyperparameters extensively

**Skip:**
- Support Vector Machines (SVMs)
- Naive Bayes
- Ensemble methods in detail (know they exist)

---

## 4. Deep Learning Core Concepts
**Total Time Estimate: 25-40 hours**

### 4.1 Neural Network Fundamentals ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Critical)
**Depth:** Master deeply - implement from scratch | **Time:** 10-15 hours  
**Why:** Everything builds on this. You must understand this viscerally, not just conceptually.

**Core Subtopics:**

1. **Perceptron to Multi-Layer Perceptron (MLP)** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)
   - **Perceptron**: Single neuron, $y = \sigma(wx + b)$
   - **MLP**: Stack of layers, each doing $h = \sigma(Wx + b)$
   - **Universal approximation**: MLPs can approximate any function
   - **Practice:** Draw a 3-layer network, trace forward pass by hand

2. **Forward Propagation** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)
   - **Layer by layer computation**:
     ```
     Layer 1: h1 = ReLU(W1 @ x + b1)
     Layer 2: h2 = ReLU(W2 @ h1 + b2)
     Output:  y = softmax(W3 @ h2 + b3)
     ```
   - **Practice:** Implement forward pass in NumPy (no PyTorch)

3. **Activation Functions** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)
   
   **Sigmoid** (‚≠ê‚≠ê‚≠ê)
   - Formula: $\sigma(x) = \frac{1}{1 + e^{-x}}$
   - Range: (0, 1)
   - **Problem**: Vanishing gradients (saturates at 0 and 1)
   - **Use**: Output layer for binary classification
   
   **Tanh** (‚≠ê‚≠ê)
   - Formula: $\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$
   - Range: (-1, 1)
   - **Problem**: Still vanishing gradients
   - **Use**: Rarely used now
   
   **ReLU** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê) - MOST IMPORTANT
   - Formula: $f(x) = \max(0, x)$
   - **Why dominant**: No vanishing gradient, computationally cheap
   - **Problem**: Dead neurons (negative inputs ‚Üí 0 gradient forever)
   - **Use**: Default hidden layer activation
   
   **Leaky ReLU** (‚≠ê‚≠ê‚≠ê)
   - Formula: $f(x) = \max(0.01x, x)$
   - **Fix**: Addresses dead ReLU problem
   
   **GELU** (‚≠ê‚≠ê‚≠ê‚≠ê) - Used in Transformers
   - Formula: $x \cdot \Phi(x)$ (Gaussian CDF)
   - **Why**: Smooth, probabilistic, used in GPT, BERT
   
   **Softmax** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê) - Output layer
   - Formula: $\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_j e^{z_j}}$
   - **Output**: Probability distribution (sums to 1)
   - **Use**: Multi-class classification, next token prediction (LLMs)
   
   **Practice:** Implement each activation function in NumPy

4. **Loss Functions** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)
   
   **Cross-Entropy** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê) - Classification
   - **Binary**: $-[y \log(\hat{y}) + (1-y) \log(1-\hat{y})]$
   - **Categorical**: $-\sum_i y_i \log(\hat{y}_i)$
   - **Why**: Measures distributional difference
   - **Derivation**: From MLE + negative log-likelihood
   - **Use**: Image classification, LLM training
   
   **Mean Squared Error (MSE)** (‚≠ê‚≠ê‚≠ê‚≠ê) - Regression
   - Formula: $\frac{1}{n}\sum(y - \hat{y})^2$
   - **Use**: Continuous value prediction
   
   **Practice:** Derive gradient of cross-entropy w.r.t. logits

5. **Backpropagation** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê) - THE MOST CRITICAL TOPIC
   **Time:** 4-6 hours to truly understand  
   **Why:** This IS deep learning. Everything else is details.
   
   - **Core idea**: Apply chain rule backwards through network
   - **Computational graph**: Nodes = operations, edges = dependencies
   - **Forward pass**: Compute outputs and store intermediates
   - **Backward pass**: Compute gradients from output to input
   
   **Step-by-step example:**
   ```
   Forward:  z1 = W1 @ x,  h1 = relu(z1),  z2 = W2 @ h1,  L = loss(z2, y)
   Backward: dL/dW2 = dL/dz2 @ h1.T
             dL/dh1 = W2.T @ dL/dz2
             dL/dz1 = dL/dh1 * relu'(z1)  # Element-wise
             dL/dW1 = dL/dz1 @ x.T
   ```
   
   **Gradient flow:**
   - Understand how gradients "flow" backwards
   - Why some layers might have vanishing/exploding gradients
   
   **Practice (MUST DO):**
   - Implement backprop for 2-layer network from scratch in NumPy
   - Compute gradients by hand for simple network
   - Verify with numerical gradients (finite differences)

**Learning Path:**
- Week 1: 3Blue1Brown "Neural Networks" series (visual intuition)
- Week 2: Implement MLP from scratch on MNIST (no frameworks!)
- Week 3: Andrej Karpathy "micrograd" tutorial (automatic differentiation)
- Week 4: Move to PyTorch, re-implement same network

---

### 4.2 Optimization in Deep Learning ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Critical)
**Depth:** Strong understanding + practical experience | **Time:** 6-8 hours  
**Why:** What makes training actually work. Bad optimization = bad results.

**Core Subtopics:**

1. **Why SGD Variants?** (‚≠ê‚≠ê‚≠ê‚≠ê)
   - **Problem**: Plain gradient descent is too slow
   - **Saddle points**: More common than local minima in high dimensions
   - **Solution**: Add momentum, adaptive learning rates

2. **Momentum** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)
   - **Idea**: Accumulate gradients like a rolling ball
   - **Formula**: 
     ```
     v_t = Œ≤ * v_{t-1} + (1-Œ≤) * ‚àáL
     Œ∏_t = Œ∏_{t-1} - Œ± * v_t
     ```
   - **Œ≤**: Typically 0.9 (keep 90% of previous velocity)
   - **Why**: Smooths noisy gradients, accelerates in consistent directions
   - **Analogy**: Ball rolling down hill gains momentum

3. **Adaptive Learning Rates** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)
   - **Problem**: Different parameters need different step sizes
   - **Example**: Rare features need larger updates than common features
   
   **RMSProp** (‚≠ê‚≠ê‚≠ê‚≠ê)
   - **Idea**: Divide by moving average of squared gradients
   - **Formula**: 
     ```
     s_t = Œ≤ * s_{t-1} + (1-Œ≤) * (‚àáL)¬≤
     Œ∏_t = Œ∏_{t-1} - Œ± * ‚àáL / ‚àö(s_t + Œµ)
     ```
   - **Effect**: Larger gradients ‚Üí smaller steps
   
   **Adam** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê) - MOST POPULAR
   - **Idea**: Combine momentum + RMSProp
   - **Maintains**: 
     - m_t (first moment, momentum)
     - v_t (second moment, RMSProp)
   - **Formula**: 
     ```
     m_t = Œ≤1 * m_{t-1} + (1-Œ≤1) * ‚àáL
     v_t = Œ≤2 * v_{t-1} + (1-Œ≤2) * (‚àáL)¬≤
     Œ∏_t = Œ∏_{t-1} - Œ± * m_t / ‚àö(v_t + Œµ)
     ```
   - **Typical values**: Œ≤1=0.9, Œ≤2=0.999, Œ±=0.001
   
   **AdamW** (‚≠ê‚≠ê‚≠ê‚≠ê) - Used in modern LLMs
   - **Fix**: Proper weight decay (add L2 penalty directly to weights)
   - **Why**: Better generalization than Adam

4. **Learning Rate Schedules** (‚≠ê‚≠ê‚≠ê‚≠ê)
   - **Step decay**: Reduce LR by 10√ó every N epochs
   - **Warmup**: Start with small LR, increase gradually (prevents early instability)
   - **Cosine annealing**: Smooth decay following cosine curve
   - **Used in**: Transformer training (warmup + cosine)

**What to Know:**
- **Conceptual understanding** of each optimizer
- **When to use**: Adam for most tasks, SGD+momentum sometimes better for generalization
- **Don't**: Memorize exact formulas (look them up)

**Practice:**
- Train same model with SGD, Adam, AdamW - compare convergence
- Visualize learning curves

---

### 4.3 Regularization & Generalization ‚≠ê‚≠ê‚≠ê‚≠ê (Important)
**Depth:** Strong understanding | **Time:** 4-6 hours  
**Why:** Making models that work on new data, not just training data.

**Core Subtopics:**

1. **Dropout** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)
   - **How**: Randomly set neurons to 0 with probability p (typically 0.5)
   - **During training**: Force network to not rely on any single neuron
   - **During inference**: Use all neurons (scale by p)
   - **Effect**: Acts like training an ensemble
   - **Practice:** Implement dropout layer in NumPy

2. **Batch Normalization** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)
   - **Formula**: $\hat{x} = \frac{x - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}$
   - **Then**: $y = \gamma \hat{x} + \beta$ (learnable scale & shift)
   - **Why**: 
     - Stabilizes training (prevents internal covariate shift)
     - Allows higher learning rates
     - Acts as regularizer
   - **When**: After linear layer, before activation
   - **Used in**: CNNs, older architectures

3. **Layer Normalization** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)
   - **Difference from BatchNorm**: Normalize across features (not batch)
   - **Formula**: Same as BatchNorm but different axis
   - **Why preferred in Transformers**: 
     - Works with variable sequence lengths
     - No batch dependency
   - **Used in**: GPT, BERT, all modern LLMs

4. **Weight Decay / L2 Regularization** (‚≠ê‚≠ê‚≠ê‚≠ê)
   - **L2 loss**: $L_{total} = L_{task} + \lambda \sum w_i^2$
   - **Effect**: Penalize large weights ‚Üí simpler model
   - **Typical Œª**: 0.01 to 0.0001

5. **Data Augmentation** (‚≠ê‚≠ê‚≠ê‚≠ê)
   - **CV**: Random crops, flips, color jitter
   - **NLP**: Back-translation, paraphrasing
   - **Why**: Artificially increase training data variety

6. **Early Stopping** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)
   - **How**: Stop training when validation loss stops improving
   - **Why**: Prevents overfitting to training set
   - **Practice**: Always monitor validation loss

**Practice:**
- Add dropout to your MLP, see effect on validation performance
- Compare model with/without batch norm

---

### 4.4 Practical Training Techniques ‚≠ê‚≠ê‚≠ê‚≠ê
**Depth:** Practical knowledge | **Time:** 3-5 hours  
**Why:** Bridging theory and practice - what actually makes training work.

**Core Subtopics:**

1. **Weight Initialization** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)
   - **Problem**: Bad init ‚Üí vanishing/exploding gradients
   - **Random isn't enough**: Need proper scale
   - **Xavier/Glorot**: $w \sim \mathcal{N}(0, \frac{2}{n_{in} + n_{out}})$ (for Tanh)
   - **He initialization**: $w \sim \mathcal{N}(0, \frac{2}{n_{in}})$ (for ReLU)
   - **Why it matters**: Proper gradient flow from the start

2. **Gradient Clipping** (‚≠ê‚≠ê‚≠ê‚≠ê)
   - **Problem**: Exploding gradients (common in RNNs, Transformers)
   - **Solution**: Clip gradient norm to max value
   - **How**: `if ||g|| > threshold: g = g * threshold / ||g||`
   - **Typical threshold**: 1.0 to 5.0

3. **Batch Size Effects** (‚≠ê‚≠ê‚≠ê‚≠ê)
   - **Small batches (32)**: Noisy gradients, regularization effect, less memory
   - **Large batches (512+)**: Stable gradients, faster on GPUs, more memory
   - **Sweet spot**: 64-256 for most tasks
   - **LLMs**: Often use very large effective batch sizes (via gradient accumulation)

4. **Interpreting Loss Curves** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)
   - **Both train & val decreasing**: Good! Keep training
   - **Train ‚Üì, val ‚Üë**: Overfitting - add regularization
   - **Both high**: Underfitting - increase capacity
   - **Train loss not decreasing**: Bug, bad LR, or bad initialization
   - **Practice:** Look at 100+ loss curves to build intuition

5. **Debugging Neural Networks** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)
   - **NaN losses**: 
     - Check for division by zero
     - Learning rate too high
     - Numerical instability in softmax
   - **Dead ReLUs**: All activations = 0
   - **Gradient vanishing**: Check gradient norms per layer
   - **Not learning**: 
     - Verify data pipeline (are labels correct?)
     - Overfit to 1 batch first
     - Check weight initialization

**Practice:**
- Deliberately break a model, debug it
- Train model with bad initialization, observe consequences

---

## 5. Architecture-Specific Prerequisites

### 5.1 For Computer Vision (CNNs) ‚≠ê‚≠ê (Only if CV is your focus)
**Depth:** Conceptual understanding | **Time:** 4-6 hours  
**Skip if focusing only on LLMs**

**Core Subtopics:**

1. **Convolution Operation** (‚≠ê‚≠ê‚≠ê)
   - **Sliding window**: Kernel slides over image
   - **Output dimension**: $O = \frac{W - K + 2P}{S} + 1$
     - W=Input size, K=Kernel size, P=Padding, S=Stride
   - **Example**: 32√ó32 image, 3√ó3 kernel, stride 1 ‚Üí 30√ó30 output
   - **Practice:** Compute by hand once

2. **Pooling** (‚≠ê‚≠ê)
   - **Max pooling**: Take max in window (most common)
   - **Average pooling**: Take mean
   - **Purpose**: Downsampling, translation invariance

3. **Spatial Invariance** (‚≠ê‚≠ê)
   - **Translation equivariance**: Shift input ‚Üí shift output
   - **Why**: Cat in top-left or bottom-right both detected

**Key Architectures (Survey):**
- ResNet (residual connections)
- VGG, EfficientNet
- **Don't implement** - use pre-trained models

---

### 5.2 For NLP/LLMs (Transformers) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (CRITICAL for your goal)
**Depth:** Master deeply - implement attention from scratch | **Time:** 20-30 hours  
**Why:** This is the foundation of ALL modern LLMs (GPT, Claude, Gemini, LLaMA).

**Part A: Text Preprocessing & Representation**

1. **Tokenization** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)  
   **Time:** 4-6 hours  
   **Why:** Garbage in = garbage out. You MUST understand this.
   
   **Word-level Tokenization** (‚≠ê‚≠ê) - Outdated
   - Split on spaces: "Hello world" ‚Üí ["Hello", "world"]
   - **Problem**: Huge vocabulary, can't handle unknown words
   
   **Character-level** (‚≠ê‚≠ê) - Rarely used
   - "Hello" ‚Üí ["H", "e", "l", "l", "o"]
   - **Problem**: Very long sequences, hard to learn
   
   **Subword Tokenization** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê) - STANDARD
   
   **Byte-Pair Encoding (BPE)** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)
   - **How it works:**
     1. Start with characters
     2. Merge most frequent pairs iteratively
     3. "tokenization" ‚Üí ["token", "ization"] or ["token", "iz", "ation"]
   - **Used in**: GPT models
   - **Practice:** Implement BPE algorithm from scratch
   
   **WordPiece** (‚≠ê‚≠ê‚≠ê‚≠ê)
   - Similar to BPE but merges based on likelihood
   - **Used in**: BERT
   
   **SentencePiece** (‚≠ê‚≠ê‚≠ê‚≠ê)
   - Language-independent (works on raw bytes)
   - **Used in**: T5, LLaMA
   
   **Key Concepts:**
   - **Vocabulary size**: Typically 30k-50k tokens
   - **Out-of-Vocabulary (OOV)**: Subword tokenization handles this
   - **Special tokens**: [CLS], [SEP], [PAD], [UNK], [MASK]
   - **Trade-offs**: Large vocab ‚Üí more tokens per word, small vocab ‚Üí longer sequences

2. **Embeddings** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)  
   **Time:** 3-4 hours  
   **Why:** How we represent discrete tokens as continuous vectors.
   
   **Word Embeddings** (‚≠ê‚≠ê‚≠ê) - Background
   - **Word2Vec**: "king" - "man" + "woman" ‚âà "queen"
   - **GloVe**: Pre-trained embeddings
   - **Problem**: Same word, same embedding (no context)
   
   **Learned Embeddings** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê) - Used in Transformers
   - **Embedding matrix**: $E \in \mathbb{R}^{V \times d}$
     - V = vocabulary size (50k)
     - d = embedding dimension (768 for BERT-base, 4096 for GPT-3)
   - **Lookup**: token_id ‚Üí row in embedding matrix
   - **Learned during training**: Initialized randomly, optimized via backprop
   - **Example:** Token "cat" (id=1337) ‚Üí $E[1337]$ ‚Üí vector in $\mathbb{R}^{768}$
   
   **Positional Encodings** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)
   - **Why needed**: Transformers have NO notion of order
   - **Solution**: Add position information to embeddings
   
   **Sinusoidal** (‚≠ê‚≠ê‚≠ê‚≠ê) - Original Transformer
   - $PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d})$
   - $PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d})$
   - **Advantage**: Generalizes to longer sequences
   
   **Learned** (‚≠ê‚≠ê‚≠ê‚≠ê) - Modern LLMs
   - Position embeddings learned during training
   - **Used in**: GPT, BERT
   
   **Rotary (RoPE)** (‚≠ê‚≠ê‚≠ê) - Advanced
   - Used in LLaMA, Mistral
   - **Don't deep-dive yet** - learn after mastering basics

**Part B: Sequence Modeling Evolution**

3. **Why Not RNNs?** (‚≠ê‚≠ê‚≠ê)  
   **Time:** 1-2 hours  
   **Understanding the motivation for Transformers**
   
   - **RNN problems**:
     - **Sequential processing**: Can't parallelize
     - **Vanishing gradients**: Hard to learn long dependencies
     - **Bottleneck**: Hidden state must encode entire history
   
   - **Why Transformers won**:
     - **Parallel processing**: Process all tokens simultaneously
     - **Direct connections**: Attention connects any token to any other
     - **Scalability**: Can train on massive data efficiently

**Part C: The Transformer Architecture**

4. **Attention Mechanism** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê) **MOST CRITICAL**  
   **Time:** 8-12 hours  
   **Why:** This is THE breakthrough. You must master this completely.
   
   **Scaled Dot-Product Attention** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)
   
   $$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$
   
   **Step-by-step breakdown:**
   
   1. **Inputs**: Sequence of vectors $X \in \mathbb{R}^{n \times d}$
      - n = sequence length
      - d = embedding dimension
   
   2. **Linear projections**:
      - $Q = XW_Q$ (Query: "what am I looking for?")
      - $K = XW_K$ (Key: "what do I contain?")
      - $V = XW_V$ (Value: "what do I actually offer?")
   
   3. **Compute attention scores**:
      - $\text{scores} = QK^T \in \mathbb{R}^{n \times n}$
      - Each element = similarity between query i and key j
   
   4. **Scale**:
      - Divide by $\sqrt{d_k}$
      - **Why**: Prevent softmax saturation (large values ‚Üí gradient ~0)
   
   5. **Softmax**: Convert scores to probabilities (rows sum to 1)
   
   6. **Weighted sum**:
      - Multiply attention weights by values
      - Each output = weighted average of all values
   
   **Intuition:**
   - "The cat sat on the mat" - processing "sat"
   - **Query** ("sat"): What words are relevant to me?
   - Computes similarity with all **Keys**
   - **High attention** to "cat" (subject) and "mat" (object)
   - **Output** for "sat" = weighted mix of values, emphasizing "cat" and "mat"
   
   **Practice (MUST DO):**
   - Implement attention from scratch in NumPy (no masking first)
   - Visualize attention weights as heatmap
   - Compute attention for "The cat sat on the mat" by hand (small d_k)
   
   **Multi-Head Attention** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)
   - **Idea**: Run attention multiple times in parallel with different learned projections
   - **Why**: Each head can learn different relationships
     - Head 1: Subject-verb
     - Head 2: Verb-object  
     - Head 3: Adjective-noun
   - **Formula**:
     ```
     head_i = Attention(Q_i, K_i, V_i)
     MultiHead = Concat(head_1, ..., head_h) W_O
     ```
   - **Typical**: h=8 for BERT-base, h=32 for large models
   
   **Self-Attention** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)
   - **Definition**: Q, K, V all come from same input X
   - **Effect**: Each position attends to all positions in same sequence
   - **Used in**: Encoder (BERT), Decoder (GPT)
   
   **Cross-Attention** (‚≠ê‚≠ê‚≠ê‚≠ê)
   - **Definition**: Q from one sequence, K and V from another
   - **Example**: Translation - Q from target, K/V from source
   - **Used in**: Encoder-Decoder models (original Transformer)

5. **Masked Attention** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)  
   **Why**: For autoregressive generation (LLMs)
   
   **Causal Masking**:
   - **Purpose**: Token i can only attend to tokens ‚â§ i
   - **Why**: Prevent "looking into the future" during training
   - **Implementation**: Set attention scores to -‚àû for future tokens
   - **Example**: Processing "The cat sat"
     - "The" can only see "The"
     - "cat" can see "The", "cat"
     - "sat" can see "The", "cat", "sat"
   
   **Mask matrix** (for length 4):
   ```
   [[1, 0, 0, 0],
    [1, 1, 0, 0],
    [1, 1, 1, 0],
    [1, 1, 1, 1]]
   ```
   
   **Practice:** Implement causal masking in your attention code

6. **Transformer Architecture** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)  
   **Time:** 4-6 hours  
   **Understanding the full picture**
   
   **Encoder-Decoder** (‚≠ê‚≠ê‚≠ê‚≠ê) - Original Transformer
   - **Used for**: Translation, summarization
   - **Encoder**: Bidirectional self-attention (sees full input)
   - **Decoder**: Causal self-attention + cross-attention to encoder
   - **Example**: T5, BART
   
   **Decoder-Only** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê) - GPT Architecture
   - **Most important for LLMs**
   - **Structure**:
     ```
     Input tokens ‚Üí Embeddings ‚Üí Positional encoding
     ‚Üí [Masked Self-Attention ‚Üí Add&Norm ‚Üí FFN ‚Üí Add&Norm] √ó N layers
     ‚Üí Output logits
     ```
   - **N**: 12 (GPT-2), 96 (GPT-3), 80+ (GPT-4 rumored)
   - **Used in**: GPT-3, GPT-4, LLaMA, Mistral, Claude
   
   **Encoder-Only** (‚≠ê‚≠ê‚≠ê) - BERT Architecture  
   - **Used for**: Classification, embeddings, understanding
   - **Bidirectional**: Can see future tokens
   - **Not generative**: Can't generate text autoregressively
   
   **Feed-Forward Networks in Transformers** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)
   - **Structure**: Two linear layers with activation
     ```
     FFN(x) = ReLU(xW_1 + b_1)W_2 + b_2
     ```
   - **Typical dimensions**: 
     - Input/output: 768 (BERT-base)
     - Hidden: 3072 (4√ó expansion)
   - **Applied**: Per-position (same FFN for each token)
   
   **Residual Connections & Layer Norm** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)
   - **Residual**: $\text{LayerNorm}(x + \text{Attention}(x))$
   - **Why**:
     - Helps gradient flow (backprop through identity)
     - Allows very deep models (100+ layers)
   - **Layer Norm**: Normalize across features
     - Applied after attention and FFN

**Learning Path for Transformers:**
- Day 1-2: Read "Attention is All You Need" paper
- Day 3-5: Jay Alammar's "Illustrated Transformer" blog (multiple times)
- Day 6-10: Implement attention mechanism from scratch
- Day 11-15: Implement full Transformer encoder block
- Day 16-20: Fine-tune pre-trained BERT/GPT with Hugging Face
- Day 21+: Andrej Karpathy "Let's build GPT" video (code along)

---

### 5.3 Generative AI Specifics ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Your End Goal)
**Depth:** Master completely | **Time:** 15-20 hours  
**Why:** This is where everything comes together for LLMs.

**Core Subtopics:**

1. **Autoregressive Generation** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)  
   **Time:** 3-4 hours
   
   - **Core idea**: Generate one token at a time, left-to-right
   - **Probability**: $P(x_1, ..., x_n) = \prod_{t=1}^n P(x_t | x_{<t})$
   - **Process**:
     ```
     Input: "The cat"
     Model outputs: P(next token | "The cat")
     Sample: "sat"
     New input: "The cat sat"
     Repeat...
     ```
   
   **Sampling Strategies** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)
   
   **Greedy Decoding** (‚≠ê‚≠ê‚≠ê)
   - Always pick highest probability token
   - **Pro**: Deterministic, fast
   - **Con**: Repetitive, boring outputs
   
   **Beam Search** (‚≠ê‚≠ê‚≠ê)
   - Keep top-k sequences, expand each
   - **Pro**: Better than greedy
   - **Con**: Still repetitive, slow
   
   **Top-k Sampling** (‚≠ê‚≠ê‚≠ê‚≠ê)
   - Sample from top k highest probability tokens
   - k=40 common
   - **Pro**: More diverse than greedy
   
   **Nucleus (Top-p) Sampling** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê) - MOST USED
   - Sample from smallest set of tokens whose cumulative prob ‚â• p
   - p=0.9 common
   - **Pro**: Adaptive (more tokens when uncertain, fewer when confident)
   - **Used in**: ChatGPT, Claude

2. **Temperature Scaling** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)  
   **Time:** 1-2 hours
   
   - **Modified softmax**: $P(x_i) = \frac{e^{z_i/T}}{\sum_j e^{z_j/T}}$
   - **Temperature = 1**: Normal softmax
   - **T < 1** (0.7): More confident, deterministic
     - Use for: Factual questions, code generation
   - **T > 1** (1.2): More random, creative
     - Use for: Creative writing, brainstorming
   - **T ‚Üí 0**: Argmax (greedy)
   - **T ‚Üí ‚àû**: Uniform distribution
   
   **Practice:** Generate text with T=0.5, 1.0, 2.0 - observe differences

3. **Evaluation Metrics** (‚≠ê‚≠ê‚≠ê‚≠ê)  
   **Time:** 2-3 hours
   
   **Perplexity** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)
   - **Formula**: $PP = \exp(-\frac{1}{N}\sum \log P(x_i|x_{<i}))$
   - **Interpretation**: "How surprised is the model?"
   - Lower = better (1 is perfect, higher is worse)
   - **Problem**: Doesn't measure usefulness
   
   **BLEU** (‚≠ê‚≠ê‚≠ê) - Translation
   - Measures n-gram overlap with reference
   - **Problem**: Doesn't capture meaning
   
   **ROUGE** (‚≠ê‚≠ê‚≠ê) - Summarization
   - Recall-oriented n-gram matching
   
   **Human Evaluation** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)
   - **Still the gold standard**
   - Helpfulness, harmlessness, honesty
   - **Challenge**: Expensive, slow
   
   **Benchmark Datasets** (‚≠ê‚≠ê‚≠ê‚≠ê)
   - **MMLU**: Multitask knowledge (57 subjects)
   - **HumanEval**: Code generation
   - **TruthfulQA**: Truthfulness
   - **HellaSwag**: Common-sense reasoning

4. **Prompting** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)  
   **Time:** 3-4 hours
   
   **Zero-shot** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)
   - **No examples**, just instruction
   - "Translate to French: Hello"
   - **Requires**: Large, well-trained model
   
   **Few-shot** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)
   - **Provide examples** in prompt
   - ```
     Translate:
     Hello ‚Üí Bonjour
     Goodbye ‚Üí Au revoir
     Thank you ‚Üí
     ```
   - **Why it works**: In-context learning
   
   **Chain-of-Thought (CoT)** (‚≠ê‚≠ê‚≠ê‚≠ê)
   - Prompt model to "think step-by-step"
   - Dramatically improves reasoning
   - "Let's solve this step by step:"

5. **Pre-training vs Fine-tuning** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)  
   **Time:** 3-4 hours
   
   **Pre-training** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)
   - **What**: Train on massive unlabeled data
   - **Task**: Next token prediction
   - **Data**: Books, web pages, code (trillions of tokens)
   - **Cost**: Millions of dollars, thousands of GPUs
   - **Result**: Base model with general knowledge
   - **You won't do this** - use pre-trained models
   
   **Fine-tuning** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)
   - **What**: Adapt pre-trained model to specific task
   - **Data**: Smaller task-specific dataset (thousands of examples)
   - **Cost**: Affordable (single GPU)
   - **Types**:
     - **Full fine-tuning**: Update all parameters
     - **LoRA** (Low-Rank Adaptation): Update small adapter layers
     - **Prompt tuning**: Learn soft prompts
   - **You WILL do this**
   
   **Instruction Tuning** (‚≠ê‚≠ê‚≠ê‚≠ê)
   - Fine-tune on (instruction, response) pairs
   - Makes model follow instructions better
   - **Example data**: "Summarize this: ..." ‚Üí summary
   
   **RLHF** (‚≠ê‚≠ê‚≠ê)
   - **What**: Reinforcement learning from human feedback
   - **Process**:
     1. Collect human preferences (A better than B)
     2. Train reward model
     3. Optimize LLM using PPO
   - **Purpose**: Align model with human values
   - **Used in**: ChatGPT, Claude, Gemini
   - **Depth needed**: Conceptual (don't implement yet)

**Emerging Topics (Learn as You Progress):**

- **Diffusion Models** (‚≠ê‚≠ê) - Images (Stable Diffusion, DALL-E 3)
- **VAEs** (‚≠ê‚≠ê) - Older generative models
- **GANs** (‚≠ê) - Less common now
- **Mixture of Experts (MoE)** (‚≠ê‚≠ê‚≠ê) - Scaling technique

---

## 6. Hardware & Systems Awareness
**Total Time Estimate: 4-8 hours**

### 6.1 Computational Fundamentals ‚≠ê‚≠ê‚≠ê (Important)
**Depth:** Conceptual understanding | **Time:** 3-5 hours  
**Why:** Understanding limitations and optimization opportunities. Won't make you faster coder but helps debug performance issues.

**Core Subtopics:**

1. **GPU vs CPU** (‚≠ê‚≠ê‚≠ê‚≠ê)
   - **CPU**: Few powerful cores (4-16), good for sequential tasks
   - **GPU**: Thousands of weak cores (5000+), massive parallelism
   - **Why GPUs for DL**: Matrix multiplication = highly parallel
   - **Example**: 
     - CPU: 1 chef making 1000 sandwiches sequentially
     - GPU: 1000 chefs making 1 sandwich each simultaneously

2. **Floating Point Precision** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)  
   **Critical for deployment**
   
   **FP32 (Float32)** (‚≠ê‚≠ê‚≠ê‚≠ê)
   - **Bits**: 32 bits per number
   - **Range**: ¬±3.4 √ó 10¬≥‚Å∏
   - **Use**: Default training precision
   - **Memory**: 4 bytes/parameter
   - **Example**: 1B parameter model = 4GB memory (just weights)
   
   **FP16 (Half precision)** (‚≠ê‚≠ê‚≠ê‚≠ê)
   - **Bits**: 16 bits per number
   - **Range**: ¬±65,504 (small!)
   - **Memory**: 2 bytes/parameter (50% savings)
   - **Speed**: 2-3√ó faster on modern GPUs
   - **Problem**: Potential numerical instability
   - **Use**: Mixed precision training
   
   **BF16 (bfloat16)** (‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê)
   - **Bits**: 16 bits (8 exponent, 7 mantissa)
   - **Range**: Same as FP32 (¬±3.4 √ó 10¬≥‚Å∏)
   - **Precision**: Lower than FP16
   - **Advantage**: No underflow issues
   - **Use**: Default for modern training (PyTorch, TPUs)
   
   **INT8 (Quantization)** (‚≠ê‚≠ê‚≠ê‚≠ê)
   - **Bits**: 8 bits (integers -128 to 127)
   - **Memory**: 1 byte/parameter (75% savings)
   - **Speed**: Even faster inference
   - **Use**: Deployment, inference optimization
   - **Trade-off**: Slight accuracy loss (usually acceptable)
   
   **Practical impact:**
   - 7B parameter LLM:
     - FP32: 28 GB
     - FP16/BF16: 14 GB
     - INT8: 7 GB
   - **Why it matters**: Difference between running on your laptop or needing cloud GPU

3. **Memory Hierarchy** (‚≠ê‚≠ê‚≠ê)
   - **GPU Memory (VRAM)**: Fast, limited (12-80 GB)
   - **System RAM**: Slower, larger (64-256 GB)
   - **Disk**: Very slow, huge (TBs)
   - **Bottleneck**: Moving data between levels
   - **Practical**: Keep active data on GPU

4. **Batch Size & Memory** (‚≠ê‚≠ê‚≠ê‚≠ê)
   - **Memory usage** ‚âà model size + activations + gradients
   - **Larger batch**: More memory but faster training (better GPU utilization)
   - **Smaller batch**: Less memory but slower
   - **Trade-off**: Batch size 32-256 usually optimal
   - **If OOM (Out of Memory)**:
     - Reduce batch size
     - Use gradient accumulation
     - Use gradient checkpointing

5. **Model Size Estimation** (‚≠ê‚≠ê‚≠ê‚≠ê)
   - **Formula**: Parameters √ó bytes per parameter
   - **Example**: GPT-3 (175B parameters)
     - FP32: 175B √ó 4 bytes = 700 GB
     - FP16: 350 GB
   - **Training memory**: ~4√ó model size (model + gradients + optimizer states)

**Good to Have:**

6. **Mixed Precision Training** (‚≠ê‚≠ê‚≠ê)
   - **Idea**: Use FP16 for speed, FP32 for numerical stability
   - **How**: 
     - Store weights in FP32
     - Compute in FP16
     - Update weights in FP32
   - **Benefit**: 2-3√ó speedup with minimal accuracy loss
   - **Implementation**: One line in PyTorch (`autocast`)

7. **Gradient Accumulation** (‚≠ê‚≠ê‚≠ê)
   - **Problem**: Can't fit large batch in memory
   - **Solution**: Accumulate gradients over multiple small batches
   - **Example**:
     - Want batch 256, can only fit 64
     - Accumulate over 4 steps, then update
   - **Effective batch size**: Still 256

8. **Gradient Checkpointing** (‚≠ê‚≠ê)
   - **Problem**: Storing all activations for backprop uses memory
   - **Solution**: Recompute activations during backward pass
   - **Trade-off**: More compute, less memory
   - **When to use**: Training very deep models

**Learning Path:**
- Read blog posts on GPU computing basics
- Experiment with different batch sizes, monitor memory usage
- Try mixed precision training (PyTorch AMP)

**Skip for Now:**
- Kernel optimization
- CUDA programming
- Detailed hardware architecture

---

### 6.2 Distributed Training Concepts ‚≠ê (Advanced - Learn Later)
**Depth:** Awareness only | **Time:** 1-2 hours  
**Why:** Large models don't fit on one GPU. You won't implement this initially but should know it exists.

**Conceptual Understanding:**

1. **Data Parallelism** (‚≠ê‚≠ê)
   - **How**: Same model copy on each GPU, different data
   - **Steps**:
     1. Replicate model on each GPU
     2. Each GPU processes different batch
     3. Average gradients across GPUs
     4. Update all models
   - **Use**: When model fits on one GPU but want faster training
   - **Tools**: PyTorch DDP (DistributedDataParallel)

2. **Model Parallelism** (‚≠ê‚≠ê)
   - **How**: Split model layers across GPUs
   - **Example**: Layers 1-10 on GPU 1, Layers 11-20 on GPU 2
   - **Problem**: Sequential (GPU 2 waits for GPU 1)
   - **Use**: When model doesn't fit on one GPU

3. **Pipeline Parallelism** (‚≠ê)
   - **How**: Split model into stages, pipeline micro-batches
   - **Reduces**: Idle time in model parallelism
   - **Complex**: Hard to implement correctly

4. **Tensor Parallelism** (‚≠ê)
   - **How**: Split individual layers across GPUs
   - **Example**: Split large matrix multiplication
   - **Used in**: Megatron-LM (NVIDIA)

**When to Learn:**
- After comfortable with single-GPU training
- When fine-tuning models >7B parameters
- When joining research lab or company

**Don't Implement Yet:** This is advanced. Focus on mastering single-GPU first.

---

## 7. **MISSING TOPICS FROM YOUR LIST** ‚ö†Ô∏è

### 7.1 Loss Functions (Critical Addition)
**Why:** You mentioned Cross-Entropy but missed others.

**Must-Know:**
- Binary Cross-Entropy vs Categorical Cross-Entropy
- Focal Loss (for imbalanced data)
- Contrastive Loss (for embeddings, e.g., sentence transformers)
- Understanding when to use which loss

---

### 7.2 Transfer Learning & Pre-training (Critical for LLMs)
**Why:** You won't train LLMs from scratch.

**Must-Know:**
- **Pre-training**: Learning general features from massive data
- **Fine-tuning**: Adapting to specific tasks
- **Feature Extraction vs Fine-tuning**: Freezing vs updating layers
- **When to use pre-trained models** (almost always for LLMs)

---

### 7.3 Tokenization Deep Dive (Critical for LLMs)
**Why:** Garbage in = garbage out.

**Must-Know:**
- **Subword Tokenization**: BPE, WordPiece, Unigram
- **Out-of-Vocabulary (OOV) Handling**
- **Vocabulary Size Trade-offs**
- **Byte-Pair Encoding (BPE)**: How it works step-by-step

---

### 7.4 Evaluation Beyond Accuracy
**Why:** LLMs can't be evaluated with simple accuracy.

**Must-Know:**
- **Perplexity**: Standard LLM metric
- **BLEU, ROUGE**: For generation tasks
- **Human Evaluation**: Importance and challenges
- **Benchmark Datasets**: GLUE, SuperGLUE, MMLU

---

## 8. **UNNECESSARY TOPICS TO SKIP** ‚ùå

### From Your Original List:
1. ‚úÇÔ∏è **Matrix Calculus Details**: Learn as needed during backprop implementation
2. ‚úÇÔ∏è **Bayesian Inference**: Not used in standard DL training (unless doing Bayesian NNs)
3. ‚úÇÔ∏è **Detailed Hardware Architecture**: Memory bandwidth, parallelism specifics are advanced
4. ‚úÇÔ∏è **Spatial Invariance Deep Dive**: Only if doing CV research

---

## Suggested Learning Path

**Total Time: 12-20 weeks** (depending on prior knowledge and time commitment)

### Phase 1: Mathematical & Programming Foundations ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
**Duration:** 2-4 weeks | **Depth:** Deep understanding | **Daily commitment:** 2-3 hours

**Week 1-2: Mathematics**
- **Linear Algebra** (8-12 hours):
  - Watch: 3Blue1Brown "Essence of Linear Algebra" (full playlist)
  - Practice: NumPy exercises (matrix operations, broadcasting)
  - Implement: Matrix multiplication from scratch
  - Goal: Comfortable with matrix operations, understand einsum
  
- **Calculus** (6-10 hours):
  - Watch: 3Blue1Brown "Essence of Calculus"
  - Practice: Compute gradients by hand (simple functions)
  - Implement: Chain rule for nested functions
  - Goal: Visceral understanding of gradients and chain rule

**Week 2-3: Python & NumPy**
- **NumPy mastery** (8-12 hours):
  - Complete NumPy tutorial
  - Daily exercises: 30min of array manipulation
  - Implement: Softmax, normalize, broadcasting examples
  - Goal: NumPy feels natural, no looking up basic operations
  
- **Python basics** (if needed):
  - Review: Lists, dicts, functions, classes
  - Practice: LeetCode Easy problems (optional but helpful)

**Week 3-4: Probability & Stats**
- **Focus areas** (6-10 hours):
  - Distributions (Normal, Bernoulli, Categorical)
  - Cross-entropy derivation from MLE
  - KL divergence intuition
  - Goal: Understand loss functions conceptually

**Checkpoint:** 
- Can you multiply 2 matrices in NumPy and explain broadcasting?
- Can you derive gradient of $f(x) = (wx + b)^2$ w.r.t. $w$?
- Can you explain why we use cross-entropy for classification?

**If yes ‚Üí Phase 2. If no ‚Üí Review weak areas**

---

### Phase 2: Classical ML Essentials ‚≠ê‚≠ê‚≠ê‚≠ê
**Duration:** 1-2 weeks | **Depth:** Conceptual understanding | **Daily commitment:** 2-3 hours

**Week 5-6: Core ML Concepts**
- **Supervised Learning** (Deep focus - 4-6 hours):
  - Implement logistic regression from scratch (NumPy)
  - Train on simple dataset (Titanic, Iris)
  - Understand: Loss, gradients, train/val/test split
  
- **Unsupervised Learning** (Conceptual - 2-3 hours):
  - Run k-means on a dataset (sklearn)
  - Understand self-supervised learning concept
  - Connection to LLM pre-training
  
- **Reinforcement Learning** (Awareness - 1 hour):
  - Watch 1 intro video
  - Read 1 blog on RLHF
  - Goal: Know what it is, not how to implement
  
- **Evaluation & Regularization** (3-4 hours):
  - Implement L2 regularization
  - Compute precision, recall, F1
  - Visualize overfitting vs underfitting

**Checkpoint:**
- Can you explain supervised vs unsupervised learning?
- What's the difference between overfitting and underfitting?
- When would you use accuracy vs F1 score?

---

### Phase 3: Deep Learning Fundamentals ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
**Duration:** 3-5 weeks | **Depth:** MASTER DEEPLY | **Daily commitment:** 3-4 hours

**Week 7-8: Neural Networks from Scratch**
- **Build an MLP** (10-15 hours):
  - **NO PyTorch/TensorFlow yet!**
  - Implement: Forward pass, ReLU, softmax
  - Implement: Backpropagation (CRITICAL - spend time here)
  - Train on MNIST (achieve >90% accuracy)
  - Goal: Understand backprop viscerally
  
**Resources:**
- Andrej Karpathy "micrograd" tutorial (highly recommended)
- Stanford CS231n Assignment 1
- Implement, debug, visualize

**Week 9-10: Move to PyTorch**
- **Learn PyTorch** (8-12 hours):
  - Re-implement your MLP in PyTorch
  - Understand: nn.Module, autograd, optimizers
  - Train on CIFAR-10
  - Experiment: Different activations, optimizers, learning rates
  
- **Practical Training** (4-6 hours):
  - Implement: Dropout, batch normalization
  - Visualize: Loss curves, dead ReLUs
  - Debug: NaN losses, gradient flow
  
**Week 11: Optimization Deep Dive**
- **Optimizers** (4-6 hours):
  - Implement: SGD with momentum (from scratch once)
  - Use: Adam, AdamW in PyTorch
  - Compare: Training curves with different optimizers
  - Tune: Learning rate, weight decay

**Checkpoint:**
- Can you implement backprop for a 2-layer network by hand?
- Can you train a CNN on CIFAR-10 and get >70% accuracy?
- Can you debug a model that's not learning?

**If yes ‚Üí Phase 4 (THE BIG ONE). If no ‚Üí Spend more time here**

---

### Phase 4: Transformers & Attention ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (MOST CRITICAL)
**Duration:** 4-6 weeks | **Depth:** MASTER COMPLETELY | **Daily commitment:** 4-5 hours

**This is the heart of modern LLMs. Do NOT rush this phase.**

**Week 12-13: Understanding Attention**
- **Theory** (8-10 hours):
  - Read: "Attention is All You Need" paper (multiple times)
  - Read: Jay Alammar's "Illustrated Transformer" (multiple times)
  - Watch: Stanford CS224n Lecture 9 (Transformers)
  - Goal: Understand Q, K, V matrices intuitively
  
- **Implementation** (10-15 hours):
  - Implement: Scaled dot-product attention from scratch (NumPy)
  - Visualize: Attention weights as heatmap
  - Compute: Attention by hand for simple example
  - Implement: Multi-head attention
  - Add: Causal masking
  - **This is THE most important implementation you'll do**

**Week 14-15: Transformer Architecture**
- **Build a Transformer** (15-20 hours):
  - Implement: Full Transformer encoder block (PyTorch)
  - Include: Self-attention, FFN, layer norm, residuals
  - Implement: Positional encodings
  - Test: Train on simple sequence task
  
- **Study architectures**:
  - Decoder-only (GPT) - FOCUS HERE
  - Encoder-only (BERT) - Brief understanding
  - Encoder-decoder - Brief understanding

**Week 16-17: Hands-on with Pre-trained Models**
- **Hugging Face** (10-12 hours):
  - Load pre-trained GPT-2, BERT
  - Fine-tune: On text classification task
  - Fine-tune: On question answering
  - Experiment: Different model sizes
  
- **Tokenization Deep Dive** (4-6 hours):
  - Implement: BPE algorithm from scratch
  - Use: Hugging Face tokenizers
  - Understand: Vocabulary, special tokens, padding
  
**Week 17: Andrej Karpathy's "Let's build GPT"**
- **Code along** (8-10 hours):
  - Follow video line-by-line
  - Implement: Character-level GPT
  - Train: On Shakespeare dataset
  - Experiment: Generate text
  - **This solidifies everything**

**Checkpoint:**
- Can you explain attention mechanism to someone else?
- Can you implement multi-head attention from scratch?
- Can you fine-tune a pre-trained model on your own dataset?

**If yes ‚Üí Phase 5. If no ‚Üí Revisit transformer implementation**

---

### Phase 5: LLM Specialization & GenAI ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Ongoing)
**Duration:** 4-8 weeks initial + ongoing | **Depth:** Deep practical knowledge

**Week 18-19: Autoregressive Generation**
- **Generation strategies** (6-8 hours):
  - Implement: Greedy, top-k, nucleus sampling
  - Experiment: Different temperatures
  - Compare: Quality of generated text
  - Goal: Understand sampling trade-offs
  
- **Evaluation** (3-4 hours):
  - Compute: Perplexity of a model
  - Understand: BLEU, ROUGE (conceptually)
  - Run: Model on benchmark datasets (MMLU, HellaSwag)

**Week 20-21: Prompting & Fine-tuning**
- **Prompting** (6-8 hours):
  - Experiment: Zero-shot, few-shot, chain-of-thought
  - Build: Prompt library for different tasks
  - Optimize: Prompts for your use cases
  
- **Fine-tuning** (10-15 hours):
  - Full fine-tune: Small model (GPT-2 or BERT-base) on custom data
  - LoRA: Efficient fine-tuning on larger model
  - Instruction tuning: Create instruction-response dataset
  - Goal: Adapt model to specific domain

**Week 22-23: Advanced Topics (Pick based on interest)**
- **RLHF** (Conceptual - 4-6 hours):
  - Understand: Reward modeling, PPO
  - Read: OpenAI/Anthropic papers on RLHF
  - Goal: Know the process, not implementation details
  
- **Quantization & Deployment** (6-8 hours):
  - Quantize: Model to INT8
  - Deploy: Model with ONNX or TensorRT
  - Optimize: Inference speed
  
- **Retrieval-Augmented Generation (RAG)** (6-8 hours):
  - Build: Simple RAG system
  - Use: Vector databases (Pinecone, Weaviate)
  - Connect: LLM with external knowledge

**Week 24+: Projects & Specialization**
- **Build real applications:**
  - Fine-tune GPT for your domain (e.g., medical, legal, code)
  - Build chatbot with RAG
  - Implement evaluation pipeline
  - Contribute to open-source LLM projects
  
- **Stay current:**
  - Read: Latest papers (arxiv-sanity)
  - Follow: Top researchers on Twitter/X
  - Join: Hugging Face forums, Discord communities
  - Participate: Kaggle competitions

---

### Parallel Learning Track (Throughout All Phases)

**Software Engineering (30min-1hr weekly):**
- **Git**: Commit daily, use branches
- **Code quality**: Write modular, readable code
- **Debugging**: Master PyTorch debugger
- **Documentation**: Comment your experiments

**Paper Reading (2-3 hours weekly):**
- **Week 1-8**: Focus on learning, light reading
- **Week 9+**: Read 1-2 papers per week
- **Key papers**:
  - "Attention is All You Need"
  - "BERT: Pre-training of Deep Bidirectional Transformers"
  - "Language Models are Few-Shot Learners" (GPT-3)
  - "Training language models to follow instructions" (InstructGPT)
  - Latest: Check arxiv-sanity for recent advances

**Community Engagement (1-2 hours weekly):**
- **Reddit**: r/MachineLearning, r/LocalLLaMA
- **Twitter/X**: Follow Andrej Karpathy, Yann LeCun, etc.
- **Discord**: Hugging Face, EleutherAI servers
- **Blogging**: Write about what you learn (best way to solidify knowledge)

---

### Self-Assessment Checkpoints

**After Phase 3 (DL Fundamentals):**
- ‚úÖ Implement MLP from scratch with backprop
- ‚úÖ Train CNN on CIFAR-10 (>70% accuracy)
- ‚úÖ Explain gradient descent to a non-technical person
- ‚úÖ Debug a broken neural network

**After Phase 4 (Transformers):**
- ‚úÖ Implement multi-head attention from scratch
- ‚úÖ Fine-tune BERT on text classification
- ‚úÖ Generate coherent text with GPT-2
- ‚úÖ Explain attention mechanism with code

**After Phase 5 (LLM Specialization):**
- ‚úÖ Fine-tune LLM on custom dataset
- ‚úÖ Build RAG application
- ‚úÖ Optimize model for deployment
- ‚úÖ Read and understand recent LLM papers

---

### Time Management Tips

**If you have 2-3 hours/day:**
- Follow the week-by-week schedule above
- Complete in 20-24 weeks (5-6 months)

**If you have 5-6 hours/day (full-time):**
- Accelerate to 2√ó speed
- Complete in 10-12 weeks (3 months)
- Spend extra time on deeper projects

**If you have 1 hour/day:**
- Double all time estimates
- Complete in 40-48 weeks (~1 year)
- Focus on quality over speed

**Golden Rule:** 
**Spend 70% time coding, 20% reading, 10% watching videos**
- You learn by DOING, not watching

---

## Key Resources by Topic

### Mathematics
- **Linear Algebra**: Gilbert Strang's MIT lectures, 3Blue1Brown "Essence of Linear Algebra"
- **Calculus**: Khan Academy, 3Blue1Brown "Essence of Calculus"
- **Probability**: Harvard Stat 110 (Joe Blitzstein)

### Deep Learning
- **Course**: Fast.ai Practical Deep Learning, Stanford CS231n (CV), CS224n (NLP)
- **Book**: "Deep Learning" by Goodfellow, Bengio, Courville (reference)
- **Hands-On**: "Dive into Deep Learning" (d2l.ai) - interactive

### Transformers/LLMs
- **Paper**: "Attention is All You Need" (original Transformer)
- **Blog**: Jay Alammar's "The Illustrated Transformer"
- **Course**: Hugging Face NLP Course, Stanford CS224n
- **Hands-On**: Andrej Karpathy's "Let's build GPT" video

### Implementation
- **PyTorch**: Official tutorials, "PyTorch Deep Learning" by Udacity
- **Hugging Face**: Transformers library documentation

---

## Final Tips

1. **Don't Learn Everything Before Starting**: You'll learn 80% by building projects
2. **Implement, Don't Just Watch**: Code attention from scratch at least once
3. **Start Simple**: MNIST ‚Üí CIFAR ‚Üí Text Classification ‚Üí LLM Fine-tuning
4. **Read Papers Actively**: Implement key ideas from 2-3 foundational papers
5. **Join Communities**: Reddit r/MachineLearning, Hugging Face forums, Discord servers
6. **Focus on Transformers**: 90% of modern GenAI uses this architecture

**Your Biggest Advantages (Coming from Math):**
- ‚úÖ You can read papers and understand derivations
- ‚úÖ You won't cargo-cult techniques
- ‚úÖ You can debug mathematical issues in training

**Your Biggest Gaps to Address:**
- ‚ö†Ô∏è Software engineering practices
- ‚ö†Ô∏è Practical experience with frameworks
- ‚ö†Ô∏è Intuition for hyperparameter tuning
- ‚ö†Ô∏è Understanding of tokenization and data preprocessing

Good luck on your journey! üöÄ
